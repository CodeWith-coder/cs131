{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1060333a-8d1f-4e43-b80e-4fa6b5542b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+------------+------------+\n",
      "|passenger_count|pulocationid|dolocationid|total_amount|\n",
      "+---------------+------------+------------+------------+\n",
      "|            1.0|        68.0|        68.0|         7.3|\n",
      "|            1.0|        50.0|        42.0|       23.15|\n",
      "|            1.0|        95.0|       196.0|         9.8|\n",
      "|            1.0|       211.0|       211.0|         6.8|\n",
      "|            1.0|       237.0|       162.0|         7.8|\n",
      "|            1.0|       148.0|        37.0|        20.3|\n",
      "|            1.0|       265.0|       265.0|        0.31|\n",
      "|            1.0|       265.0|       265.0|      240.35|\n",
      "|            1.0|       237.0|       142.0|        10.3|\n",
      "|            1.0|       249.0|        69.0|        36.3|\n",
      "+---------------+------------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final one\n",
    "from pyspark.sql import SparkSession # Build a SparkSession\n",
    "from pyspark.sql import DataFrameReader # PySpark to read DataFrame\n",
    "\n",
    "# Establish w/ Apache Spark SQL interface using PySpark and initialize configurations for application\n",
    "spark = SparkSession.builder.appName(\"Taxi_Observation\").getOrCreate()\n",
    "\n",
    "dataset_path = \"gs://for-spark-0709/2019-04.csv\"\n",
    "\n",
    "# assign an instance of DataFrameReader to reader\n",
    "reader = DataFrameReader(spark)\n",
    "\n",
    "# To load CSV file info dataFrame\n",
    "# use Spark to load dataset file \"2019-04.csv\" into dataframe\n",
    "# set two options for Spark to infer schema data types and include column headers\n",
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load(dataset_path))\n",
    "\n",
    "# create DataFrame as temporary table view named \"taxi_passenger\"\n",
    "df.createOrReplaceTempView(\"taxi_passenger\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT passenger_count, pulocationid, dolocationid, total_amount\n",
    "FROM taxi_passenger\n",
    "\"\"\"\n",
    "\n",
    "# run the query via Spark SQL and print the top 10 rows\n",
    "pass_cnt_df = spark.sql(query)\n",
    "pass_cnt_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ede730-16db-4c5b-aaf6-c761f12c1c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:===========================================>              (6 + 2) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers in total dataset: 7433136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers in training set:  5946606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:==================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers in testing set:   1486530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 2. Create trainDF and testDF.\n",
    "trainDF, testDF = pass_cnt_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Numbers in total dataset: {pass_cnt_df.count()}\")\n",
    "print(f\"Numbers in training set:  {trainDF.count()}\")\n",
    "print(f\"Numbers in testing set:   {testDF.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0332a6df-08c9-4a92-97b2-9d39413fa41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+------------+------------------+\n",
      "|passenger_count|pulocationid|dolocationid|        prediction|\n",
      "+---------------+------------+------------+------------------+\n",
      "|            0.0|         1.0|         1.0|18.718776971163287|\n",
      "|            0.0|         4.0|         4.0|18.718776971163287|\n",
      "|            0.0|         4.0|        33.0|18.718776971163287|\n",
      "|            0.0|         4.0|        79.0|15.615861838189069|\n",
      "|            0.0|         4.0|       107.0|15.615861838189069|\n",
      "|            0.0|         4.0|       144.0| 17.81124095206958|\n",
      "|            0.0|         4.0|       234.0| 17.81124095206958|\n",
      "|            0.0|         7.0|       121.0| 17.81124095206958|\n",
      "|            0.0|         7.0|       223.0| 17.81124095206958|\n",
      "|            0.0|         7.0|       223.0| 17.81124095206958|\n",
      "+---------------+------------+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 3. Create a decision tree regressor to predict total_amount from the other three features.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# define the selected features to new columns and instantiate them to \"vecAssembler\" object\n",
    "columns = [\"passenger_count\", \"pulocationid\", \"dolocationid\"]\n",
    "vecAssembler = VectorAssembler( inputCols = columns, outputCol = \"features\" )\n",
    "\n",
    "# initialize a decision tree regression model and assign the selected columns featureCol and labelCol\n",
    "decisionTreeR = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"total_amount\")\n",
    "\n",
    "# 4. Create a pipeline.\n",
    "# transformation of pipeline defined by VectorAssembler and DecisionTreeRegressor\n",
    "pipeline = Pipeline(stages=[vecAssembler, decisionTreeR])\n",
    "\n",
    "# 5. Train the model.\n",
    "# train the pipline using the training dataset and save into pipeline_model\n",
    "pipeline_model = pipeline.fit(trainDF)\n",
    "\n",
    "# Make predictions for the given test data.\n",
    "predictions_df = pipeline_model.transform(testDF)\n",
    "\n",
    "# saves the predictions into a temporary SQL view\n",
    "predictions_df.createOrReplaceTempView(\"results\")\n",
    "\n",
    "# 6. Show the predicted results along with the three features in the notebook. (Show the first 10 entries.)\n",
    "# execute SQL query on temporary SQL view \"result\" with selected columns showing first 10 rows of dataFrame\n",
    "spark.sql(\"\"\"\n",
    "SELECT passenger_count, pulocationid, dolocationid, prediction\n",
    "FROM results\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba26e42a-a9d0-4aa1-a37d-cd13cb8ea3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:=============================>                            (4 + 4) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Root Mean Squared Error): 11.8675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 7. Evaluate the model with RMSE\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# RegressionEvaluator objects calculates the RMSE between predicted value\n",
    "# and real values of selected columns\n",
    "regress_evaluator = RegressionEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=\"total_amount\",\n",
    "    metricName=\"rmse\")\n",
    "\n",
    "# evaluate the RMSE for regression model by given real value and predicted value\n",
    "rmse = regress_evaluator.evaluate(predictions_df)\n",
    "rmse_f = round(rmse, 4) # round up the RMSE value up to four decimal places\n",
    "print(f\"RMSE (Root Mean Squared Error): {rmse_f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2 on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-runtime-0000de74c485"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
